{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e9714b7",
   "metadata": {},
   "source": [
    "### This project aims to predict the size category of a company based on the alumniâ€™s professional data, such as their industry, company specialties, and LinkedIn connection count.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2671747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# PostgreSQL Connection\n",
    "engine = create_engine(\"postgresql+psycopg2://postgres:postgres123@localhost:5432/DW\")\n",
    "\n",
    "# Loading the data\n",
    "query = \"\"\"\n",
    "SELECT f.\"factKey\", f.\"dateFK\", f.\"AluminiFK\", f.\"CompanyFK\", f.\"num_of_connections\",\n",
    "       a.\"first_name\", a.\"last_name\", a.\"Industry\", a.\"Current_Position\",\n",
    "       c.\"company_size\", c.\"company_size_category\", c.\"company_specialties\"\n",
    "FROM \"Fact_Employability\" f\n",
    "JOIN \"Dim_Alumini\" a ON f.\"AluminiFK\" = a.\"AluminiKey\"\n",
    "JOIN \"Dim_Company\" c ON f.\"CompanyFK\" = c.\"CompanyKey\"\n",
    "\"\"\"\n",
    "conn = engine.raw_connection()\n",
    "df = pd.read_sql(query, conn)\n",
    "conn.close()\n",
    "\n",
    "# Data Cleaning\n",
    "df['num_of_connections'] = pd.to_numeric(df['num_of_connections'], errors='coerce')\n",
    "df = df.dropna(subset=['num_of_connections'])\n",
    "df = df.drop(columns=[\"factKey\", \"first_name\", \"last_name\", \"Current_Position\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b9b5400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp/ipykernel_14208/745332196.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['num_of_connections'] = pd.to_numeric(X['num_of_connections'], errors='coerce')\n",
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š RandomForest - Accuracy: 77.69%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        1-10       0.00      0.00      0.00         3\n",
      "      10000+       0.57      0.99      0.73        70\n",
      "   1001-2000       1.00      0.89      0.94        64\n",
      "   1001-5000       1.00      0.78      0.88        37\n",
      "       11-50       1.00      0.18      0.31        11\n",
      "     201-500       1.00      0.30      0.46        10\n",
      "  5001-10000       1.00      0.20      0.33         5\n",
      "    501-1000       1.00      0.60      0.75         5\n",
      "      51-200       0.89      0.65      0.75        37\n",
      "\n",
      "    accuracy                           0.78       242\n",
      "   macro avg       0.83      0.51      0.57       242\n",
      "weighted avg       0.85      0.78      0.76       242\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š GradientBoosting - Accuracy: 82.23%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        1-10       0.00      0.00      0.00         3\n",
      "      10000+       0.62      1.00      0.77        70\n",
      "   1001-2000       1.00      0.91      0.95        64\n",
      "   1001-5000       1.00      0.78      0.88        37\n",
      "       11-50       1.00      0.45      0.62        11\n",
      "     201-500       1.00      0.50      0.67        10\n",
      "  5001-10000       1.00      0.20      0.33         5\n",
      "    501-1000       1.00      0.60      0.75         5\n",
      "      51-200       1.00      0.76      0.86        37\n",
      "\n",
      "    accuracy                           0.82       242\n",
      "   macro avg       0.85      0.58      0.65       242\n",
      "weighted avg       0.88      0.82      0.82       242\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š XGBoost - Accuracy: 72.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        1-10       0.00      0.00      0.00         3\n",
      "      10000+       0.70      0.89      0.78        70\n",
      "   1001-2000       0.93      0.84      0.89        64\n",
      "   1001-5000       0.88      0.78      0.83        37\n",
      "       11-50       0.22      0.18      0.20        11\n",
      "     201-500       0.12      0.10      0.11        10\n",
      "  5001-10000       0.00      0.00      0.00         5\n",
      "    501-1000       0.12      0.20      0.15         5\n",
      "      51-200       0.72      0.70      0.71        37\n",
      "\n",
      "    accuracy                           0.72       242\n",
      "   macro avg       0.41      0.41      0.41       242\n",
      "weighted avg       0.71      0.72      0.71       242\n",
      "\n",
      "\n",
      "ðŸŽ¯ Model Summary:\n",
      "           Model  Accuracy\n",
      "GradientBoosting  0.822314\n",
      "    RandomForest  0.776860\n",
      "         XGBoost  0.723140\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Data\n",
    "X = df[['num_of_connections', 'Industry', 'company_specialties', 'company_size']]\n",
    "y = df['company_size_category']\n",
    "\n",
    "# Data Cleaning\n",
    "X['num_of_connections'] = pd.to_numeric(X['num_of_connections'], errors='coerce')\n",
    "X = X.dropna()\n",
    "y = y.loc[X.index]\n",
    "\n",
    "# Columns\n",
    "categorical = ['Industry', 'company_specialties', 'company_size']\n",
    "numerical = ['num_of_connections']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train_str, y_test_str = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# âœ… Creating an encoded version of y for XGBoost\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_enc = label_encoder.fit_transform(y_train_str)\n",
    "y_test_enc = label_encoder.transform(y_test_str)\n",
    "\n",
    "# Results\n",
    "results = []\n",
    "\n",
    "# Model and preprocessor definitions\n",
    "model_defs = {\n",
    "    \"RandomForest\": (\n",
    "        RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "        ColumnTransformer([\n",
    "            (\"cat\", OneHotEncoder(handle_unknown='ignore', sparse=True), categorical),\n",
    "            (\"num\", StandardScaler(), numerical)\n",
    "        ]),\n",
    "        y_train_str,\n",
    "        y_test_str\n",
    "    ),\n",
    "    \"GradientBoosting\": (\n",
    "        GradientBoostingClassifier(random_state=42),\n",
    "        ColumnTransformer([\n",
    "            (\"cat\", OneHotEncoder(handle_unknown='ignore', sparse=True), categorical),\n",
    "            (\"num\", StandardScaler(), numerical)\n",
    "        ]),\n",
    "        y_train_str,\n",
    "        y_test_str\n",
    "    ),\n",
    "    \"XGBoost\": (\n",
    "        XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "        ColumnTransformer([\n",
    "            (\"cat\", OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical),\n",
    "            (\"num\", StandardScaler(), numerical)\n",
    "        ]),\n",
    "        y_train_enc,\n",
    "        y_test_enc\n",
    "    )\n",
    "}\n",
    "\n",
    "# ðŸ“Š Training Loop\n",
    "for name, (model, preprocessor, y_train_model, y_test_model) in model_defs.items():\n",
    "    pipe = Pipeline([\n",
    "        (\"prep\", preprocessor),\n",
    "        (\"clf\", model)\n",
    "    ])\n",
    "    pipe.fit(X_train, y_train_model)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test_model, y_pred)\n",
    "    print(f\"\\nðŸ“Š {name} - Accuracy: {acc:.2%}\")\n",
    "    \n",
    "    if name == \"XGBoost\":\n",
    "        print(classification_report(y_test_model, y_pred, target_names=label_encoder.classes_))\n",
    "    else:\n",
    "        print(classification_report(y_test_model, y_pred))\n",
    "    \n",
    "    results.append((name, acc))\n",
    "\n",
    "# Summary\n",
    "df_results = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\"]).sort_values(by=\"Accuracy\", ascending=False)\n",
    "print(\"\\nðŸŽ¯ Model Summary:\")\n",
    "print(df_results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be46e499",
   "metadata": {},
   "source": [
    "=> GradientBoosting has the highest accuracy at 82.23%, indicating it performed the best in classifying the data correctly.\n",
    "\n",
    "RandomForest follows with an accuracy of 77.69%, showing a good performance, though slightly lower than GradientBoosting.\n",
    "\n",
    "XGBoost has the lowest accuracy at 72.31%,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
